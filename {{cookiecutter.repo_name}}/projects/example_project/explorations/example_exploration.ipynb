{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7424fa79-1079-4dfc-90a4-9d5af09c7812",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Example Data Exploration\n",
    "\n",
    "**Purpose**: Ad-hoc analysis and data exploration template\n",
    "\n",
    "**Author**: {TEAM NAME OR INDIVIDUAL}\n",
    "\n",
    "**Date**: {LAST UPDATE DATE}\n",
    "\n",
    "## Overview\n",
    "Use this notebook for:\n",
    "- Exploring new data sources\n",
    "- Prototyping transformations\n",
    "- Analyzing data quality\n",
    "- Creating visualizations\n",
    "- Testing utility functions\n",
    "\n",
    "## Performance Strategy for Exploration\n",
    "\n",
    "**Key Principles:**\n",
    "1. **Filter first, sample later** - Apply date filters and LIMIT before any operations\n",
    "2. **Use recent data only** - Start with last 7-30 days, not full history\n",
    "3. **Hard limits** - Cap at 10k-100k rows for exploration\n",
    "4. **Handle corrupt files** - Configure Spark to skip bad data\n",
    "5. **Approximate counts** - Use `approx_count_distinct()` for speed\n",
    "6. **Avoid full scans** - Never call `.count()` on unfiltered large tables\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: If you get import errors after updating utility functions, restart the Python kernel:\n",
    "- In Databricks: Click **Run** → **Clear state and run all**\n",
    "- Or use: `dbutils.library.restartPython()` (will restart kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3dfb606-34eb-4d3e-8930-bbd1b8c00ea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Add projects to path\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in dir() else os.getcwd()\n",
    "projects_path = os.path.abspath(os.path.join(current_dir, \"..\", \"..\"))\n",
    "if projects_path not in sys.path:\n",
    "    sys.path.insert(0, projects_path)\n",
    "\n",
    "from src.environment_config import EnvironmentConfig\n",
    "\n",
    "# Import utility functions for exploration\n",
    "from example_project.utilities import (\n",
    "    check_data_quality,\n",
    "    get_date_range,\n",
    "    sample_by_date,\n",
    "    categorize_sport_udf\n",
    ")\n",
    "\n",
    "print(\"✓ Setup complete - utilities imported\")\n",
    "print(f\"✓ Available utilities: sample_by_date, check_data_quality, get_date_range, categorize_sport_udf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c88b1b07-96fe-4eb0-9719-cc330394c075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize config with defaults for exploration\n",
    "spark.conf.set(\"bundle.catalog\", \"sandbox\")\n",
    "spark.conf.set(\"bundle.schema\", \"analytics_engineering\")\n",
    "spark.conf.set(\"bundle.core_catalog\", \"core_views\")\n",
    "\n",
    "config = EnvironmentConfig()\n",
    "print(f\"Environment: {config.env}\")\n",
    "print(f\"Source Catalog: {config.core_catalog}\")\n",
    "print(f\"Output schema: {config.project_schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e53e3c14-e670-4272-9a8a-db03cf7de9df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Read and Sample Data\n",
    "\n",
    "# Configure Spark to handle corrupt files gracefully\n",
    "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")\n",
    "spark.conf.set(\"spark.sql.files.ignoreMissingFiles\", \"true\")\n",
    "\n",
    "# Use utility function to sample by date for fast exploration\n",
    "source_table = config.get_core_table_path(\"sportsbook\", \"bet_legs\")\n",
    "\n",
    "# Sample last 7 days with max 10k rows using utility function\n",
    "df = sample_by_date(\n",
    "    table_path=source_table,\n",
    "    date_column=\"bet_placed_local_ts\",\n",
    "    days_back=7,\n",
    "    max_rows=10000\n",
    ")\n",
    "\n",
    "print(f\"✓ Sampled data using sample_by_date utility\")\n",
    "print(f\"  - Last 7 days\")\n",
    "print(f\"  - Max 10,000 rows\")\n",
    "print(f\"  - Actual rows: {df.count():,}\")\n",
    "print(f\"\\nSchema (first 10 columns):\")\n",
    "df.select(df.columns[:10]).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a264354-c94b-4781-a1d4-22ed108c83e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Explore Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afc12199-7a11-4b80-ba72-1b6e4c50fcec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data quality checks using utility function\n",
    "print(\"=== DATA QUALITY ANALYSIS ===\")\n",
    "\n",
    "# Use utility function for comprehensive quality checks\n",
    "quality_metrics = check_data_quality(\n",
    "    df,\n",
    "    columns_to_check=['bet_id', 'bet_placed_local_ts', 'leg_sport_name_reporting', 'bet_portion']\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Quality check complete using check_data_quality() utility\")\n",
    "\n",
    "# Get date range using utility function\n",
    "date_range = get_date_range(df, 'bet_placed_local_ts')\n",
    "print(f\"\\n=== DATE RANGE ===\")\n",
    "print(f\"From: {date_range['min_date']}\")\n",
    "print(f\"To: {date_range['max_date']}\")\n",
    "\n",
    "# Quick preview of data\n",
    "print(\"\\n=== SAMPLE DATA ===\")\n",
    "display(df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6887a9b-b138-4298-bcba-8d462affa8e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prototype Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6acace9e-8232-4b02-8edc-317df337a70d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fast aggregations for exploration\n",
    "print(\"=== TOP SPORTS BY BET COUNT ===\")\n",
    "\n",
    "# Direct Spark aggregations - clear and performant\n",
    "sport_summary = df.groupBy(\"leg_sport_name_reporting\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"bet_count\"),\n",
    "        F.sum(\"bet_portion\").alias(\"total_stake\"),\n",
    "        F.avg(\"bet_portion\").alias(\"avg_stake\")\n",
    "    ) \\\n",
    "    .orderBy(F.desc(\"bet_count\")) \\\n",
    "    .limit(10)  # Top 10 only\n",
    "\n",
    "display(sport_summary)\n",
    "\n",
    "# Test the sport categorization UDF\n",
    "print(\"\\n=== SPORT CATEGORIZATION (using UDF utility) ===\")\n",
    "categorized_df = df.select(\n",
    "    'leg_sport_name_reporting',\n",
    "    categorize_sport_udf(F.col('leg_sport_name_reporting')).alias('sport_category')\n",
    ").distinct().orderBy('sport_category').limit(20)\n",
    "\n",
    "display(categorized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b07c358a-8e9b-4c8c-b2f4-403bcf4a58b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "**Optimized Utility Functions Demonstrated:**\n",
    "\n",
    "✅ **`sample_by_date()`** - Fast data sampling with date filter and row limit\n",
    "  - Uses SQL predicate pushdown for optimal performance\n",
    "  - Applies LIMIT at query level\n",
    "  - Perfect for exploration without loading full datasets\n",
    "\n",
    "✅ **`check_data_quality()`** - Single-pass quality checks\n",
    "  - Optimized: One aggregation query for all null counts\n",
    "  - Checks first 10 columns by default (configurable)\n",
    "  - Shows null percentages for context\n",
    "\n",
    "✅ **`get_date_range()`** - Extract date ranges efficiently\n",
    "  - Single aggregation for min/max dates\n",
    "\n",
    "✅ **`categorize_sport_udf()`** - Custom UDF for transformations\n",
    "  - Demonstrates domain-specific logic as reusable function\n",
    "\n",
    "**Performance Best Practices:**\n",
    "\n",
    "1. **Filter first with `sample_by_date()`** - Always start with recent, limited data\n",
    "2. **Single-pass aggregations** - Combine multiple metrics in one query\n",
    "3. **Direct Spark functions** - Use `.groupBy().agg()` for simple aggregations\n",
    "4. **Limit results** - Always use `.limit()` for top-N queries\n",
    "5. **Handle corrupt files** - Set Spark configs to skip bad data\n",
    "\n",
    "**When to Use Utilities vs Direct Code:**\n",
    "\n",
    "| Use Utility Function | Use Direct Spark Code |\n",
    "|---------------------|----------------------|\n",
    "| Common patterns (sampling, quality checks) | One-off aggregations |\n",
    "| Complex logic (UDFs, validations) | Simple groupBy operations |\n",
    "| Reusable across projects | Notebook-specific analysis |\n",
    "| Needs optimization (single-pass) | Already optimal |\n",
    "\n",
    "**Exploration Workflow:**\n",
    "```python\n",
    "# 1. Sample with utility\n",
    "df = sample_by_date(table_path, \"date_col\", days_back=7, max_rows=10000)\n",
    "\n",
    "# 2. Quality check with utility\n",
    "metrics = check_data_quality(df, columns_to_check=['col1', 'col2'])\n",
    "\n",
    "# 3. Direct aggregations for analysis\n",
    "summary = df.groupBy(\"category\").agg(F.count(\"*\"), F.sum(\"amount\")).limit(10)\n",
    "\n",
    "# 4. Use UDFs for complex transformations\n",
    "df_transformed = df.withColumn(\"category\", my_udf(F.col(\"raw_value\")))\n",
    "```\n",
    "\n",
    "**Document your findings here:**\n",
    "- Key insights from the data\n",
    "- Data quality issues discovered\n",
    "- Transformation ideas to implement\n",
    "- Questions for stakeholders"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "example_exploration",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
