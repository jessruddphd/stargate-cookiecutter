{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a93d277-cfce-49cc-b1f8-64378d4f2296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Example Transformation Pipeline\n",
    "\n",
    "**Purpose**: Demonstrate best practices for data transformations using EnvironmentConfig\n",
    "\n",
    "**Author**: {TEAM NAME OR INDIVIDUAL}\n",
    "\n",
    "**Last Updated**: {LAST UPDATE DATE}\n",
    "\n",
    "## Overview\n",
    "This notebook shows how to:\n",
    "- Set up environment configuration\n",
    "- Read from source tables\n",
    "- Transform data\n",
    "- Write to output tables\n",
    "- Handle errors gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c1ff699-4336-4aed-8203-c35744cb0e4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b119b78-4d8c-49a7-82eb-cd899d758808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add projects directory to Python path\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in dir() else os.getcwd()\n",
    "projects_path = os.path.abspath(os.path.join(current_dir, \"..\", \"..\"))\n",
    "\n",
    "if projects_path not in sys.path:\n",
    "    sys.path.insert(0, projects_path)\n",
    "\n",
    "print(f\"Added to Python path: {projects_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c7cab86-f4a4-4a99-bf11-023311c86869",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure Spark for optimal performance\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")  # Handle corrupt files\n",
    "spark.conf.set(\"spark.sql.files.ignoreMissingFiles\", \"true\")  # Handle missing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fc6c562-7a3b-4aff-996a-09a9cf0f163d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set Spark config so EnvironmentConfig can read these values\n",
    "spark.conf.set(\"bundle.catalog\", \"sandbox\")\n",
    "spark.conf.set(\"bundle.schema\", \"analytics_engineering\")\n",
    "spark.conf.set(\"bundle.core_catalog\", \"core_views\")\n",
    "\n",
    "print(\"✓ Spark configuration set for sandbox environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8129c187-0f60-43c0-9a52-6494106408ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize EnvironmentConfig\n",
    "from src.environment_config import EnvironmentConfig\n",
    "\n",
    "config = EnvironmentConfig()\n",
    "print(f\"Environment Config: {config}\")\n",
    "\n",
    "# Import utility functions from this project\n",
    "from example_project.utilities import (\n",
    "    validate_dataframe,\n",
    "    check_data_quality,\n",
    "    add_audit_columns,\n",
    "    get_date_range,\n",
    "    categorize_sport_udf\n",
    ")\n",
    "\n",
    "print(\"✓ Utility functions imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ae5a959-229b-49f1-9568-cc53da53a3d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Read Source Data\n",
    "\n",
    "Use `config.get_core_table_path()` to get source tables from the core catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01b7b946-b065-4656-8f83-b940517e78dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define source tables using EnvironmentConfig\n",
    "source_table = config.get_core_table_path(\"sportsbook\", \"bet_legs\")\n",
    "print(f\"Reading from: {source_table}\")\n",
    "\n",
    "# Read data with filters - aggregation pushdown for performance\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    bet_placed_local_ts AS bet_date,\n",
    "    leg_sport_name_reporting AS sport_name,\n",
    "    leg_competition_name_reporting AS competition_name,\n",
    "    leg_market_name_reporting AS market_name,\n",
    "    COUNT(DISTINCT bet_id) AS bet_count,\n",
    "    SUM(bet_portion) AS total_stake\n",
    "FROM {source_table}\n",
    "WHERE bet_placed_local_ts >= current_date() - 30\n",
    "GROUP BY \n",
    "    bet_placed_local_ts,\n",
    "    leg_sport_name_reporting,\n",
    "    leg_competition_name_reporting,\n",
    "    leg_market_name_reporting\n",
    "\"\"\"\n",
    "\n",
    "df_source = spark.sql(query)\n",
    "\n",
    "# Validate the source data using utility function\n",
    "is_valid = validate_dataframe(\n",
    "    df_source,\n",
    "    required_columns=['bet_date', 'sport_name', 'market_name', 'bet_count'],\n",
    "    min_rows=1\n",
    ")\n",
    "\n",
    "if not is_valid:\n",
    "    raise ValueError(\"Source data validation failed!\")\n",
    "\n",
    "print(f\"✓ Source data validated: {df_source.count():,} rows\")\n",
    "display(df_source.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b859365a-eb49-445b-becb-70e33f6fe162",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Transform Data\n",
    "\n",
    "Apply business logic and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c2833e3-451f-4b05-a21d-18359ace3a50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply transformations - all in one pass for efficiency\n",
    "df_transformed = df_source \\\n",
    "    .withColumn(\"sport_name_clean\", F.lower(F.trim(F.col(\"sport_name\")))) \\\n",
    "    .withColumn(\"competition_name_clean\", F.lower(F.trim(F.col(\"competition_name\")))) \\\n",
    "    .withColumn(\"market_name_clean\", F.lower(F.trim(F.col(\"market_name\")))) \\\n",
    "    .withColumn(\"avg_stake\", F.col(\"total_stake\") / F.col(\"bet_count\")) \\\n",
    "    .withColumn(\"sport_category\", categorize_sport_udf(F.lower(F.trim(F.col(\"sport_name\"))))) \\\n",
    "    .withColumn(\"processed_timestamp\", F.current_timestamp())\n",
    "\n",
    "# Use optimized utility function for data quality checks (single-pass aggregation)\n",
    "print(\"=== DATA QUALITY CHECKS ===\")\n",
    "quality_metrics = check_data_quality(\n",
    "    df_transformed,\n",
    "    columns_to_check=['sport_name_clean', 'bet_count', 'total_stake', 'avg_stake']\n",
    ")\n",
    "\n",
    "# Additional validation\n",
    "if quality_metrics['total_nulls'] > 0:\n",
    "    print(f\"⚠️  WARNING: Found {quality_metrics['total_nulls']} null values\")\n",
    "else:\n",
    "    print(\"✓ No null values in key columns\")\n",
    "\n",
    "print(f\"✓ Transformation complete: {quality_metrics['total_rows']:,} rows\")\n",
    "display(df_transformed.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00b70756-422d-401c-a9fa-e99e32705850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Check for Existing Data\n",
    "\n",
    "Handle the case where the output table may not exist yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cccd05a-43e1-4cea-8a4a-532c7de3aa31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define output table\n",
    "output_table = config.get_table_path(\"example_betting_summary\")\n",
    "print(f\"Output table: {output_table}\")\n",
    "\n",
    "# Check if table exists and get summary stats\n",
    "print(\"=== CHECKING EXISTING DATA ===\")\n",
    "try:\n",
    "    existing_df = spark.table(output_table)\n",
    "    \n",
    "    # Use single aggregation for count and date range (optimized)\n",
    "    stats = existing_df.agg(\n",
    "        F.count(\"*\").alias(\"row_count\"),\n",
    "        F.min(\"bet_date\").alias(\"min_date\"),\n",
    "        F.max(\"bet_date\").alias(\"max_date\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    existing_count = stats['row_count']\n",
    "    print(f\"Existing rows: {existing_count:,}\")\n",
    "    print(f\"Date range: {stats['min_date']} to {stats['max_date']}\")\n",
    "    \n",
    "except AnalysisException as ex:\n",
    "    if \"TABLE_OR_VIEW_NOT_FOUND\" in str(ex):\n",
    "        print(\"✓ Table does not exist yet. This is the first run.\")\n",
    "        existing_count = 0\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a1c3f6e-e58a-427b-ae51-e2e29cdae5f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Write Output Data\n",
    "\n",
    "Use staging table pattern for safe updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c621d46-cfd8-4eb7-b49d-5ac509d8233a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define staging table\n",
    "staging_table = config.get_table_path(\"example_betting_summary_staging\")\n",
    "print(f\"Staging table: {staging_table}\")\n",
    "\n",
    "# Add audit columns using utility function\n",
    "df_with_audit = add_audit_columns(df_transformed, user=\"databricks_job\")\n",
    "\n",
    "# Write to staging - use optimal partitioning\n",
    "print(\"=== WRITING TO STAGING ===\")\n",
    "df_with_audit.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(staging_table)\n",
    "\n",
    "staging_count = spark.table(staging_table).count()\n",
    "print(f\"✓ Written {staging_count:,} rows to staging table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4219057-d3fd-43d0-bba9-bfb4d3e9f20f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create or replace final table\n",
    "print(\"=== UPDATING FINAL TABLE ===\")\n",
    "\n",
    "# For this example, always do full refresh (simpler and often faster for small tables)\n",
    "# For large tables with incremental updates, use MERGE instead\n",
    "print(\"Performing full refresh...\")\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {output_table}\n",
    "    AS SELECT * FROM {staging_table}\n",
    "\"\"\")\n",
    "\n",
    "# Clean up staging table\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {staging_table}\")\n",
    "print(\"✓ Final table updated and staging cleaned up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baff1949-d1b8-42b4-b74b-c32b976abff4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Verify Results\n",
    "\n",
    "Check the final output and display summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a78e664-2e04-4dd7-bc8c-4c15be70eec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get final stats - single aggregation for efficiency\n",
    "print(\"=== FINAL RESULTS ===\")\n",
    "final_df = spark.table(output_table)\n",
    "\n",
    "# Single aggregation for all summary stats\n",
    "summary_stats = final_df.agg(\n",
    "    F.count(\"*\").alias(\"total_rows\"),\n",
    "    F.sum(\"bet_count\").alias(\"total_bets\"),\n",
    "    F.sum(\"total_stake\").alias(\"total_stake\"),\n",
    "    F.min(\"bet_date\").alias(\"min_date\"),\n",
    "    F.max(\"bet_date\").alias(\"max_date\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"Total rows: {summary_stats['total_rows']:,}\")\n",
    "print(f\"Total bets: {summary_stats['total_bets']:,}\")\n",
    "print(f\"Total stake: ${summary_stats['total_stake']:,.2f}\")\n",
    "print(f\"Date range: {summary_stats['min_date']} to {summary_stats['max_date']}\")\n",
    "print(f\"Rows added/updated: {summary_stats['total_rows'] - existing_count:,}\")\n",
    "\n",
    "# Show summary by sport (limited to top 10)\n",
    "print(\"\\n=== TOP 10 SPORTS BY BET COUNT ===\")\n",
    "summary_by_sport = final_df.groupBy(\"sport_name_clean\", \"sport_category\").agg(\n",
    "    F.sum(\"bet_count\").alias(\"total_bets\"),\n",
    "    F.sum(\"total_stake\").alias(\"total_stake\"),\n",
    "    F.avg(\"avg_stake\").alias(\"avg_stake_per_bet\")\n",
    ").orderBy(F.desc(\"total_bets\")).limit(10)\n",
    "\n",
    "display(summary_by_sport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78e7b8fe-0480-47ae-9be9-096d4677825c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample of final data\n",
    "print(\"\\n=== SAMPLE OUTPUT ===\")\n",
    "display(final_df.orderBy(F.desc(\"bet_date\")).limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92c0044e-e060-4976-9b52-466b396b3bd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "This transformation demonstrates **optimized best practices**:\n",
    "\n",
    "### **Performance Optimizations**\n",
    "- ✅ **Aggregation pushdown** - GROUP BY in source query reduces data scan\n",
    "- ✅ **Single-pass transformations** - All `.withColumn()` operations chained together\n",
    "- ✅ **Optimized quality checks** - `check_data_quality()` uses single aggregation\n",
    "- ✅ **Efficient stats gathering** - Combined count/min/max in one query\n",
    "- ✅ **Corrupt file handling** - Spark configs to skip bad data\n",
    "- ✅ **Adaptive query execution** - Enabled for automatic optimization\n",
    "\n",
    "### **Utility Functions Used**\n",
    "1. **`validate_dataframe()`** - Fast schema and row count validation\n",
    "2. **`check_data_quality()`** - Single-pass null checks (optimized)\n",
    "3. **`add_audit_columns()`** - Adds timestamps and user tracking\n",
    "4. **`categorize_sport_udf()`** - Custom UDF for business logic\n",
    "\n",
    "### **Best Practices**\n",
    "- ✅ Uses `EnvironmentConfig` for all table paths\n",
    "- ✅ Filters data at source (last 30 days)\n",
    "- ✅ Validates data before processing\n",
    "- ✅ Handles missing tables gracefully\n",
    "- ✅ Uses staging table pattern for safe updates\n",
    "- ✅ Provides clear logging and verification\n",
    "- ✅ Limits display results for performance\n",
    "\n",
    "### **When to Use Full Refresh vs Incremental**\n",
    "\n",
    "**Full Refresh** (used here):\n",
    "- Small to medium tables (< 100M rows)\n",
    "- Daily or less frequent updates\n",
    "- Simpler logic, easier to debug\n",
    "- Often faster than MERGE for small datasets\n",
    "\n",
    "**Incremental MERGE**:\n",
    "- Large tables (> 100M rows)\n",
    "- Frequent updates (hourly)\n",
    "- Need to preserve history\n",
    "- Append-only or upsert patterns\n",
    "\n",
    "### **Next Steps**\n",
    "1. Customize transformation logic for your use case\n",
    "2. Add project-specific utilities to `utilities/example_utils.py`\n",
    "3. Create job YAML to schedule this notebook\n",
    "4. Test in dev, then deploy to qa and production\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: If you get import errors after updating utilities, restart the Python kernel:\n",
    "- In Databricks: **Run** → **Clear state and run all**"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "example_transformation",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
